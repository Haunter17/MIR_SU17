import numpy as np
import tensorflow as tf
import h5py
from sklearn.preprocessing import OneHotEncoder
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import time

startTime = time.time()

print('==> Experiment 1o - normalization')

def loadData(filepath):

	print('==> Loading data from {}'.format(filepath))
	f = h5py.File(filepath)
	X_train = np.array(f.get('trainingFeatures'))
	y_train = np.array(f.get('trainingLabels'))
	X_test = np.array(f.get('validationFeatures'))
	y_test = np.array(f.get('validationLabels'))
	del f
	print('==> Data sizes:',X_train.shape, y_train.shape, X_test.shape, y_test.shape)

	# Transform labels into on-hot encoding form
	enc = OneHotEncoder()
	y_train = enc.fit_transform(y_train.copy()).astype(int).toarray()
	y_test = enc.fit_transform(y_test.copy()).astype(int).toarray()

	return [X_train, y_train, X_test, y_test]

# Neural-network model set-up
# Functions for initializing neural nets parameters
def init_weight_variable(shape):
  initial = tf.truncated_normal(shape, stddev=0.1, dtype=tf.float64)
  return tf.Variable(initial)

def init_bias_variable(shape):
  initial = tf.constant(0.1, shape=shape, dtype=tf.float64)
  return tf.Variable(initial)


def runNeuralNet(num_features, hidden_layer_size, X_train, y_train, X_test, y_test, batchSize, numEpochs):

	'''
		NN config parameters
	'''

	num_classes = y_test.shape[1]
	
	print('==> Creating Neural net with %d features, %d hidden units, and %d classes'%(num_features, hidden_layer_size, num_classes))

	# Set-up NN layers
	x = tf.placeholder(tf.float64, [None, num_features])
	W1 = init_weight_variable([num_features, hidden_layer_size])
	b1 = init_bias_variable([hidden_layer_size])

	# Hidden layer activation function: ReLU
	h1 = tf.nn.relu(tf.matmul(x, W1) + b1)

	W2 = init_weight_variable([hidden_layer_size, num_classes])
	b2 = init_bias_variable([num_classes])

	# Softmax layer (Output), dtype = float64
	y = tf.matmul(h1, W2) + b2

	# NN desired value (labels)
	y_ = tf.placeholder(tf.float64, [None, num_classes])

	# Loss function
	cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))
	train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)

	sess = tf.InteractiveSession()

	correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
	accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64))
	sess.run(tf.global_variables_initializer())


	'''
		Training config
	'''
	numTrainingVec = len(X_train)
	print_freq = 5

	train_accuracies = []
	test_accuracies = []
	train_costs = []
	test_costs = []
	# track which epochs you grab data on
	epoch_numbers = []

	print('Training with %d samples, a batch size of %d, for %d epochs'%(numTrainingVec, batchSize, numEpochs))


	for epoch in range(numEpochs):

		epochStart = time.time()
		for i in range(0,numTrainingVec,batchSize):

			# Batch Data
			batchEndPoint = min(i+batchSize, numTrainingVec)
			trainBatchData = X_train[i:batchEndPoint]
			trainBatchLabel = y_train[i:batchEndPoint]

			train_step.run(feed_dict={x: trainBatchData, y_: trainBatchLabel})

		epochEnd = time.time()


		# Print accuracy

		if (epoch + 1) % print_freq == 0:
			# calculate the accuracies and costs at this epoch
			train_accuracy = accuracy.eval(feed_dict={x:X_train, y_: y_train})
			test_accuracy = accuracy.eval(feed_dict={x: X_test, y_: y_test})
			train_cost = cross_entropy.eval(feed_dict={x:X_train, y_: y_train})
			test_cost = cross_entropy.eval(feed_dict={x: X_test, y_: y_test})
			# update the lists
			train_accuracies += [train_accuracy]
			test_accuracies += [test_accuracy]
			train_costs += [train_cost]
			test_costs += [test_cost]
			epoch_numbers += [epoch]
			print("epoch: %d, time: %g, t acc, v acc, t cost, v cost: %g, %g, %g, %g"%(epoch+1, epochEnd - epochStart, train_accuracy, test_accuracy, train_cost, test_cost))

	# Validation
	train_accuracy = accuracy.eval(feed_dict={x:X_train, y_: y_train})
	test_accuracy = accuracy.eval(feed_dict={x: X_test, y_: y_test})
	train_cost = cross_entropy.eval(feed_dict={x:X_train, y_: y_train})
	test_cost = cross_entropy.eval(feed_dict={x:X_test, y_: y_test})
	print("test accuracy %g"%(test_accuracy))
	return [train_accuracies, test_accuracies, train_costs, test_costs, epoch_numbers]


''' 
our main  - compare regularization methods
'''


'''
 set params
'''
numEpochs = 5
batchSize = 1000

[X_train, y_train, X_test, y_test] = loadData('pylon2/ci560sp/cstrong/taylorswift_smallDataset_71_7.mat')

# normalize X_train and X_test
X_train_squared = np.square(X_train)
sumOfSquares = X_train_squared.sum(axis=1)
X_train_normalized = X_train / np.sqrt(sumOfSquares[:, np.newaxis]) # divide each row by its magnitude

X_test_squared = np.square(X_test)
sumOfSquares = X_test_squared.sum(axis=1)
X_test_normalized = X_test / np.sqrt(sumOfSquares[:, np.newaxis])

# run once without normalization then once with normalization
# un-normalized
[trainingAccuracies, testAccuracies, trainingCosts, testCosts, epochNumbers] = runNeuralNet(121, 100, X_train, y_train, X_test, y_test, batchSize, numEpochs)

# normalized
[trainingAccuraciesNormalized, testAccuraciesNormalized, trainingCostsNormalized, testCostsNormalized, epochNumbersNormalized] = runNeuralNet(121, 100, X_train_normalized, y_train, X_test_normalized, y_test, batchSize, numEpochs)


numTrainingSamples = X_train.shape[0]


endTime = time.time()
print("Whole experiment Took: %g"%(endTime - startTime))


'''
Printing results
'''
print("--------------------------")
print("Summary Of Results - exp1n, Regularization")
print("--------------------------")
print("Epoch Numbers: %s"%str(epochNumbers))
print("Training Accuracy: %s"%str(trainingAccuracies))
print("Test Accuracy: %s"%str(testAccuraciesNormalized))
print("Training Cost: %s"%str(trainingCosts))
print("Test Cost: %s"%str(testCosts))

print("Training Accuracy Normalized: %s"%str(trainingAccuraciesNormalized))
print("Test Accuracy Normalized: %s"%str(testAccuraciesNormalized))
print("Training Cost Normalized: %s"%str(trainingCostsNormalized))
print("Test Cost Normalized: %s"%str(testCostsNormalized))
'''
Plotting results
'''

matplotlib.rcParams.update({'font.size': 8})


fig = plt.figure(figsize=(8,4 * 3))

# training and validation costs during raining for the normalized data
unnormalizedPlot = fig.add_subplot(311)
unnormalizedPlot.set_xlabel("Epoch Numbers")
unnormalizedPlot.set_ylabel("Cross-Entropy Error")
unnormalizedPlot.set_title("Validation and Training Error vs. Epoch Number, Un-normalized Data")
unnormalizedPlot.plot(epochNumbers, trainingCosts, label="Training, un-normalized", marker="o", markersize="3", ls="None")
unnormalizedPlot.plot(epochNumbers, testCosts, label="validation, un-normalized", marker="o", markersize="3", ls="None")
unnormalizedPlot.legend(loc="upper right", frameon=False)


# training and validation costs during raining for the un-normalized data
normalizedPlot = fig.add_subplot(312)
normalizedPlot.set_xlabel("Epoch Numbers")
normalizedPlot.set_ylabel("Cross-Entropy Error")
normalizedPlot.set_title("Validation and Training Error vs. Epoch Number, Normalized Data")
normalizedPlot.plot(epochNumbers, trainingCostsNormalized, label="Training, normalized", marker="o", markersize="3", ls="None")
normalizedPlot.plot(epochNumbers, testCostsNormalized, label="validation, normalized", marker="o", markersize="3", ls="None")
normalizedPlot.legend(loc="upper right", frameon=False)

# validation from both runs on the same plot
bothValidationPlot = fig.add_subplot(313)
bothValidationPlot.set_xlabel("Epoch Numbers")
bothValidationPlot.set_ylabel("Cross-Entropy Error")
bothValidationPlot.set_title("Validation Error vs. Epoch Number, Both normalized and un-normalized Data")
bothValidationPlot.plot(epochNumbers, testCosts, label="Validation, un-normalized", marker="o", markersize="3", ls="None")
bothValidationPlot.plot(epochNumbers, testCostsNormalized, label="validation, normalized", marker="o", markersize="3", ls="None")
bothValidationPlot.legend(loc="upper right", frameon=False)

fig.tight_layout()
fig.savefig('exp1n_Normalization.png')


'''
Whole experiment Took: 2029.46
--------------------------
Summary Of Results - exp1n, Regularization, small TS
--------------------------
Epoch Numbers: [4, 9, 14, 19, 24, 29, 34, 39, 44, 49, 54, 59, 64, 69, 74, 79, 84, 89, 94, 99, 104, 109, 114, 119, 124, 129, 134, 139, 144, 149, 154, 159, 164, 169, 174, 179, 184, 189, 194, 199, 204, 209, 214, 219, 224, 229, 234, 239, 244, 249, 254, 259, 264, 269, 274, 279, 284, 289, 294, 299, 304, 309, 314, 319, 324, 329, 334, 339, 344, 349, 354, 359, 364, 369, 374, 379, 384, 389, 394, 399, 404, 409, 414, 419, 424, 429, 434, 439, 444, 449, 454, 459, 464, 469, 474, 479, 484, 489, 494, 499]
Training Accuracy: [0.099099341309315775, 0.15162350755844506, 0.20055725965122387, 0.23802319471092165, 0.26598883036576265, 0.2892861943809652, 0.30834300798005604, 0.32517567121680591, 0.33989905779124768, 0.35322440699507524, 0.36552811350499215, 0.37608671742291855, 0.3859145290789327, 0.39450805948991191, 0.40265431570714522, 0.40995246184115636, 0.41662246880690218, 0.42321181977049699, 0.42921213750626264, 0.43497048723557652, 0.44009580955406968, 0.44509892580869853, 0.44993584181647078, 0.45448923975607625, 0.45883488738711181, 0.46296056410319036, 0.46684427281281665, 0.47025626611592425, 0.47391267154676175, 0.47735888254775155, 0.48045802832736512, 0.48324188246220756, 0.48659277273338269, 0.48966258905766868, 0.49230468415842726, 0.49471703185912047, 0.49736890344498852, 0.49985213066272316, 0.50241845800388574, 0.50459372594068064, 0.50681298806046704, 0.50900780896748044, 0.51126128878514876, 0.51314326216866823, 0.51527453592247197, 0.51744491561671213, 0.51928045069596285, 0.52111109753265894, 0.52295885321829616, 0.52465262926346357, 0.52643683779589123, 0.52798885480697488, 0.52959219836488214, 0.53098534749294168, 0.53228073176990964, 0.53371298683840651, 0.53522589790905384, 0.53650661745835782, 0.53790709895024924, 0.53912671546762092, 0.5405491940510081, 0.54202788742377328, 0.54336482176245537, 0.54473597379901906, 0.54586515782913081, 0.5471507656209893, 0.54828483789365567, 0.5495313397450774, 0.55062875019858404, 0.55169927531804064, 0.5528382358332613, 0.55395519925698633, 0.55501839201261083, 0.55603025822141217, 0.55707389800681839, 0.55816642021777063, 0.55906830096909299, 0.56004350535873515, 0.5610724804164775, 0.56206723777633782, 0.56294956555744202, 0.56380256388321892, 0.56474843881753323, 0.56559899302203287, 0.56640310892226386, 0.56736364858424193, 0.56831685588238801, 0.56921140426987904, 0.57005707023182417, 0.57100294516613836, 0.57182661403658774, 0.57264783878575942, 0.57350572535409161, 0.57421696464578487, 0.57484021557149578, 0.57555389898446674, 0.57629202361021081, 0.57687372447420771, 0.57758251964462404, 0.57838663554485481]
Test Accuracy: [0.22119831885456884, 0.29038972007722014, 0.33281551640926632, 0.36208976833976847, 0.38391348938223951, 0.40028756435006441, 0.4135798745173746, 0.42563042953667973, 0.43509692728442761, 0.44269827863577887, 0.44984214124839145, 0.45613135456885495, 0.46147542631917682, 0.46615588803088842, 0.47037886100386134, 0.47478784588159634, 0.47863879504504547, 0.48241433397683442, 0.48555642696267748, 0.48856278153153221, 0.49127252252252318, 0.4938314430501935, 0.49615910553410608, 0.49886381917631978, 0.5011613175675681, 0.50299629987130046, 0.50513292310167368, 0.50688746782496841, 0.50877775096525146, 0.51045185810810856, 0.51222148487773556, 0.51401122104247166, 0.51535855051480117, 0.51702763030888099, 0.51848556145431213, 0.51977256274131334, 0.52103442728442806, 0.52221585424710482, 0.5235129102316608, 0.52468428249678323, 0.52610702220077288, 0.52717282014157096, 0.52832408301158362, 0.52969654922779985, 0.53086792149292239, 0.53205940315315403, 0.53313525579150656, 0.53396476833976902, 0.53509592181467269, 0.53625221203346252, 0.53712194337194419, 0.53810227638352737, 0.5390273085585594, 0.53998753217503281, 0.54074666184041276, 0.54142032657657735, 0.54208393661518739, 0.54297880469755555, 0.5438736727799236, 0.54482886904761985, 0.5454623149935659, 0.5459901866151875, 0.54638231981982044, 0.54677445302445393, 0.54760396557271629, 0.54830276705276793, 0.54906692406692481, 0.5496350144787655, 0.55014780405405495, 0.55087676962677046, 0.55136944980695068, 0.55192748552123638, 0.55263634169884246, 0.55313404922780018, 0.55384290540540615, 0.55453667953668029, 0.55499416827541914, 0.55563766891891964, 0.55627111486486558, 0.55667330276705385, 0.55721625643500727, 0.55761844433719521, 0.55809101512226589, 0.55864402348777453, 0.55913167631917726, 0.5595338642213652, 0.55992599742599825, 0.56049408783783872, 0.56092643983269064, 0.56147944819819917, 0.56188163610038699, 0.56221344111969185, 0.56264076576576671, 0.56307814510939569, 0.56338984073359166, 0.56380208333333426, 0.56420929858429925, 0.56470700611325708, 0.56518460424710537, 0.56560690154440241]
Training Cost: [3.8447487923804666, 3.3455197372746488, 3.0813820058306183, 2.9049885134124604, 2.7772029469897652, 2.6784852747642955, 2.5979938828501661, 2.5297309597399527, 2.470346810210458, 2.4178780510962019, 2.3710343508009659, 2.3288631272391629, 2.2906289849910246, 2.2557710145175545, 2.2238262373686659, 2.1944015518246163, 2.1671840246166307, 2.1419155658637945, 2.1183694939407314, 2.0963306300085209, 2.0756241061561651, 2.0561070213818393, 2.0376475598200683, 2.0201468002106164, 2.0034981291212497, 1.9876211672123372, 1.9724457215790181, 1.9579019563342328, 1.9439546815405166, 1.9305636498765719, 1.9176734818127099, 1.9052489292893175, 1.8932644481752239, 1.8816869972620813, 1.8704975205565841, 1.8596698515570103, 1.8492037050236247, 1.839083408273005, 1.8292748496179452, 1.8197686229729573, 1.8105408895176884, 1.8015834124862347, 1.7928803551140455, 1.7844189325488555, 1.7761912113580776, 1.7681852123343527, 1.7603935773670321, 1.7528026134860413, 1.7454113134361926, 1.7382101568577315, 1.7311964277465055, 1.7243509892837878, 1.7176625855072667, 1.7111393394066143, 1.7047800013784569, 1.6985715504867376, 1.6925158354722589, 1.6865994845605188, 1.6808111724654209, 1.675156091823172, 1.6696234464081654, 1.6642063038806121, 1.6588990735592366, 1.6536974152981669, 1.6486033253073362, 1.6436067047369676, 1.6387050057394543, 1.6339021832083327, 1.6291861888863988, 1.6245574781728704, 1.6200176658482059, 1.6155628788497787, 1.6111950478837496, 1.606908813749325, 1.6027036692024632, 1.5985715475991864, 1.5945125453214983, 1.5905240689319267, 1.5866034538318896, 1.5827468657278607, 1.5789507039182384, 1.5752174823099752, 1.5715452085002632, 1.5679337111318883, 1.5643805874444501, 1.5608816559983656, 1.5574364520025046, 1.5540443338355043, 1.5507031327219818, 1.5474145086915509, 1.5441785227390341, 1.5409904499808909, 1.5378456150517468, 1.534743385392257, 1.5316880742077181, 1.5286789311259421, 1.5257121011889356, 1.5227824165735655, 1.5198883754145884, 1.5170292105418941]
Test Cost: [3.8387927078450068, 3.3386242844615244, 3.0817512843689001, 2.9128566778993874, 2.7912806696307597, 2.6978884688423737, 2.6222470064144643, 2.5586235544278009, 2.5037976486862012, 2.455786102891135, 2.4132474965392552, 2.375196118534737, 2.3408623449571349, 2.3096671668491551, 2.2811582424943762, 2.2549877496631856, 2.2308660601896153, 2.2085457093677361, 2.1878087808429032, 2.1684446447181664, 2.1503236951739328, 2.1332842676543526, 2.11720873115451, 2.1020146982500245, 2.0875976716142701, 2.0738814463881003, 2.060814358137339, 2.0483497794990422, 2.0364659935755385, 2.0250596448020444, 2.0141256213914698, 2.0036306156982335, 1.9935280418309349, 1.983794644187949, 1.9744187831485687, 1.9653646919405194, 1.9566521998835502, 1.9482369997192661, 1.9401100935416749, 1.9322573791488071, 1.9246386057470413, 1.9172472312243265, 1.9100768841073088, 1.9031126129131275, 1.8963562753155287, 1.8897855967460513, 1.8833957922469544, 1.8771769355689469, 1.8711384793328851, 1.8652564298931769, 1.8595422506091011, 1.8539699394657441, 1.848532791016833, 1.8432317857415486, 1.8380695170738648, 1.8330403603151226, 1.8281349020404507, 1.823346803445254, 1.8186595040565494, 1.8140861625669593, 1.8096218196599794, 1.8052506787310358, 1.8009701555707494, 1.796782388796224, 1.7927115422760058, 1.7887217161320517, 1.7848061051882289, 1.7809863816584408, 1.777212712113811, 1.7735241233999106, 1.7699319871341685, 1.7664082415402127, 1.7629693777999331, 1.759606182241056, 1.7563191089685228, 1.7530997410025706, 1.7499412996080816, 1.7468418905008398, 1.7438135716615861, 1.7408435698013331, 1.7379348500093819, 1.7350872483417727, 1.7322982434067833, 1.7295755869658707, 1.7268837659327017, 1.724244288844472, 1.7216443920406186, 1.7190807638589123, 1.7165678454020676, 1.7140802901841279, 1.711643376608138, 1.7092551699235055, 1.7068942581213493, 1.7045640516244029, 1.7022885005893011, 1.7000385255798049, 1.6978310524828761, 1.6956388468778405, 1.6934964711742597, 1.6913796645295445]
Training Accuracy Normalized: [0.23249459238167411, 0.30546627723667658, 0.35000305515159658, 0.38226056776937278, 0.40699263097434885, 0.42578547947549145, 0.44130076134377755, 0.4543450366007159, 0.46563198865927691, 0.47502474672793238, 0.48295592027276346, 0.48995588361094422, 0.49622994292976752, 0.50206406041867746, 0.50739468892446382, 0.5124344669982519, 0.51678255875056456, 0.52084957655598818, 0.52459641447408556, 0.52829192584535967, 0.53145217465690586, 0.53481528553446755, 0.53776289579488901, 0.54089625927238438, 0.54364345158806737, 0.54614378765474259, 0.54882498869593832, 0.55098314778379209, 0.55321707463124259, 0.55541189553825576, 0.55742585147074908, 0.55943736328196547, 0.56136821909103052, 0.56345305454056571, 0.56529347786237061, 0.56692615087560561, 0.56861748279949575, 0.5702843735106129, 0.57184861112808338, 0.57357660487113282, 0.57500152757579748, 0.57663175646775511, 0.57818621760011635, 0.57951826369624349, 0.58083808918598434, 0.58220190885871659, 0.58342152537608827, 0.58476090383604729, 0.58613694411516637, 0.5874812108176799, 0.58884747461168907, 0.59004998228012007, 0.59135758716347397, 0.59273607156386976, 0.59388969680675452, 0.59515575162839496, 0.59632892984149788, 0.59752166102481918, 0.59864351269109839, 0.59971159368927773, 0.60067457747253294, 0.60155934937491506, 0.60267142455608547, 0.60377127913086948, 0.60484180425032563, 0.60574124088037151, 0.606811765999828, 0.6077063143873187, 0.6085104302875497, 0.60942697576653626, 0.61033130063913665, 0.61111341944787168, 0.61211795329284102, 0.61297828398245036, 0.6138581676422773, 0.61465250705739927, 0.6155397230810582, 0.61630473304085232, 0.61706729887936917, 0.61785675005193641, 0.61866819831599917, 0.6194967554290034, 0.62015911229515097, 0.62089723692089482, 0.62169646457857142, 0.62241014799154237, 0.62318249031516826, 0.62375197057277909, 0.62459763653472378, 0.62529176697747635, 0.62584658250742287, 0.62646983343313412, 0.62711996969289496, 0.62764789988879111, 0.62832980972515751, 0.62889440174021327, 0.62952742915103332, 0.62996003861711491, 0.63044153050874274, 0.63107211379828565]
Test Accuracy Normalized: [0.22119831885456884, 0.29038972007722014, 0.33281551640926632, 0.36208976833976847, 0.38391348938223951, 0.40028756435006441, 0.4135798745173746, 0.42563042953667973, 0.43509692728442761, 0.44269827863577887, 0.44984214124839145, 0.45613135456885495, 0.46147542631917682, 0.46615588803088842, 0.47037886100386134, 0.47478784588159634, 0.47863879504504547, 0.48241433397683442, 0.48555642696267748, 0.48856278153153221, 0.49127252252252318, 0.4938314430501935, 0.49615910553410608, 0.49886381917631978, 0.5011613175675681, 0.50299629987130046, 0.50513292310167368, 0.50688746782496841, 0.50877775096525146, 0.51045185810810856, 0.51222148487773556, 0.51401122104247166, 0.51535855051480117, 0.51702763030888099, 0.51848556145431213, 0.51977256274131334, 0.52103442728442806, 0.52221585424710482, 0.5235129102316608, 0.52468428249678323, 0.52610702220077288, 0.52717282014157096, 0.52832408301158362, 0.52969654922779985, 0.53086792149292239, 0.53205940315315403, 0.53313525579150656, 0.53396476833976902, 0.53509592181467269, 0.53625221203346252, 0.53712194337194419, 0.53810227638352737, 0.5390273085585594, 0.53998753217503281, 0.54074666184041276, 0.54142032657657735, 0.54208393661518739, 0.54297880469755555, 0.5438736727799236, 0.54482886904761985, 0.5454623149935659, 0.5459901866151875, 0.54638231981982044, 0.54677445302445393, 0.54760396557271629, 0.54830276705276793, 0.54906692406692481, 0.5496350144787655, 0.55014780405405495, 0.55087676962677046, 0.55136944980695068, 0.55192748552123638, 0.55263634169884246, 0.55313404922780018, 0.55384290540540615, 0.55453667953668029, 0.55499416827541914, 0.55563766891891964, 0.55627111486486558, 0.55667330276705385, 0.55721625643500727, 0.55761844433719521, 0.55809101512226589, 0.55864402348777453, 0.55913167631917726, 0.5595338642213652, 0.55992599742599825, 0.56049408783783872, 0.56092643983269064, 0.56147944819819917, 0.56188163610038699, 0.56221344111969185, 0.56264076576576671, 0.56307814510939569, 0.56338984073359166, 0.56380208333333426, 0.56420929858429925, 0.56470700611325708, 0.56518460424710537, 0.56560690154440241]
Training Cost Normalized: [3.036377633279844, 2.6148603618204054, 2.4146749484215806, 2.2831163913469688, 2.1864748078093466, 2.1117248726666449, 2.0518982871572446, 2.002768370717114, 1.9615295459560516, 1.9262677787226292, 1.8956388512898164, 1.8686262168416627, 1.8445216714403987, 1.8227670199052222, 1.8029526744922475, 1.7847298246860692, 1.7678823857267556, 1.752215552175393, 1.7375847836461531, 1.7238582275930552, 1.7109339472386837, 1.698720092512491, 1.6871472393519211, 1.6761434335539283, 1.6656525255183443, 1.6556116103646334, 1.6460087290956695, 1.6368033902831338, 1.6279703941114918, 1.6194591485536292, 1.6112479836367648, 1.6033159807869322, 1.5956394721706897, 1.588184507260084, 1.5809399225131893, 1.5739039354384776, 1.5670727303777787, 1.5604279464237907, 1.5539549965561483, 1.5476254531398266, 1.5414342283481162, 1.5353849227165395, 1.5294590742346312, 1.5236631837605286, 1.517993393115789, 1.5124382912148742, 1.5069821497709184, 1.501616814086921, 1.4963489324746468, 1.4911738799168366, 1.4860928697147255, 1.4811047639597001, 1.4762077570180663, 1.4713914251722549, 1.4666727152699559, 1.4620584163541286, 1.4575295565215556, 1.4530834859468054, 1.4487154938713909, 1.4444218453154598, 1.4401989441127263, 1.436050169711117, 1.4319812404764618, 1.4279895579823698, 1.4240665265239352, 1.4202029747631018, 1.4164030196279476, 1.4126711614763972, 1.4089981477151521, 1.4053879787458177, 1.4018293533765573, 1.3983256889157205, 1.3948758848590908, 1.3914814770125048, 1.388137624892781, 1.3848478239720321, 1.3816066342335025, 1.3784196079814308, 1.3752884152212628, 1.3722069229777212, 1.3691751508692447, 1.3661926755002503, 1.3632536774115989, 1.3603604564810468, 1.3575110365910743, 1.3547078382230429, 1.3519454356603067, 1.3492220257475405, 1.3465382593636315, 1.3438906154537686, 1.3412863000692525, 1.3387236523958936, 1.3361989297509067, 1.3337133564658221, 1.3312610822010502, 1.3288462696429133, 1.326464139775609, 1.3241132634103099, 1.3217919718488109, 1.3195014502002911]
Test Cost Normalized: [3.0566065423767044, 2.6514701741648894, 2.4626879441249181, 2.3421766068772651, 2.2559497573672376, 2.1904285179222041, 2.1385647236516898, 2.0963304225781951, 2.0611952100010016, 2.0314784117471483, 2.0059754520166768, 1.9837761588681659, 1.9642070517857235, 1.9467325995885683, 1.9309845481808676, 1.9166682846797025, 1.9035776950599992, 1.891530534281965, 1.8803505782449637, 1.8699371792007202, 1.8602006264951712, 1.851037754447217, 1.8423757571942392, 1.8341723681022339, 1.8263628766311426, 1.818890448369989, 1.8117642659870619, 1.804956097942011, 1.7984639873049755, 1.7922228341884956, 1.7861838689932799, 1.7803616320646916, 1.7747467171399121, 1.7692839566947125, 1.7639783606960826, 1.7588519623949515, 1.7538722733883159, 1.7490309169309228, 1.7443111330264944, 1.7396890933978786, 1.7351802926783828, 1.7308003174314921, 1.7264955654334342, 1.7223143124485558, 1.7182484612981608, 1.7142592758047319, 1.710332399171697, 1.7064857756593608, 1.7027078063728582, 1.6990119481779784, 1.695371480550373, 1.6918318843069142, 1.6883435786273056, 1.6849293265565952, 1.6815798700509954, 1.6782916729448294, 1.6750751651769438, 1.6719150596108934, 1.6688145561784435, 1.6657802479038213, 1.6627832758515344, 1.6598322758471404, 1.6569272879477774, 1.6540655933010979, 1.6512440865274229, 1.648471175544522, 1.6457601602695955, 1.6431083791045615, 1.6405135932828179, 1.6379783601974767, 1.6354625520503776, 1.6330043722758634, 1.6305865561182753, 1.628215018519213, 1.6258867201002429, 1.6235965659133007, 1.6213396577276684, 1.6191536622337805, 1.6169876657635456, 1.6148678800381873, 1.612781605828121, 1.6107202733436612, 1.6086959500882674, 1.6067047632649758, 1.6047674831722902, 1.6028747093157034, 1.6010212456160156, 1.5991940581192918, 1.5973994220215759, 1.5956159960966605, 1.5938900748048062, 1.5921973015230984, 1.5905367319251589, 1.5889389086961814, 1.5873541902656991, 1.5857835067488106, 1.5842409094720846, 1.5827199295109264, 1.5812126394515471, 1.5797209136096371]
'''




